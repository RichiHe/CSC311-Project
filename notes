X_train shape: (577, 3289)
X_val shape:   (123, 3289)
X_test shape:  (125, 3289)
totally: 825个data

X_train的三个matrix
===== TEXT MATRIX =====
Shape: (577, 3269)
每一行是一个student的data，每一列是对应的baggings of words，里面的数字代表word出现的频率。
features是按字母表顺序alphabetically排的
#可以考虑删掉一些low-frequency的单词，也可以不删。如果要删我再过滤一下我不知道删不删

===== RATING MATRIX =====
Shape: (577, 4)
[[1. 2. 2. 4.]
 [3. 2. 3. 2.]
 [1. 2. 4. 5.]
 ...
 [1. 1. 1. 4.]
 [4. 3. 3. 2.]
 [3. 3. 3. 3.]]
 每一行是一个student的data，每一列是一个问题的评分
 固定顺序对应每个Rating Column： FATING_COLS = [2, 4, 7, 8]

 ===== MULTI MATRIX =====
Shape: (577, 16)
[[0 1 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 1 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 1 0 0]
 [1 1 0 ... 0 0 1]]
每一行是一个student的data，每一列是一个option。
我没数错的话1个question有八个options，然后两个问题的options都是一样的，所以2*8 = 16
Options如下
 CANONICAL_MULTI_TYPES = [
    "Math computations",
    "Writing or debugging code",
    "Data processing or analysis",
    "Explaining complex concepts simply",
    "Writing or editing essays/reports",
    "Brainstorming or generating creative ideas",
    "Drafting professional text (e.g., résumés, emails)",
    "Converting content between formats (e.g., LaTeX)",
]

note：
前面define这个常量MULTI_MAP在pred.py开头是因为python会把
“Drafting professional text (e.g., résumés, emails)"算成三个选项，因为这里有三个逗号
映射完就只算一个了

MULTI_MAP = {
    "math computations": "Math computations",
    "writing or debugging code": "Writing or debugging code",
    "data processing or analysis": "Data processing or analysis",
    "explaining complex concepts simply": "Explaining complex concepts simply",
    "writing or editing essays/reports": "Writing or editing essays/reports",
    "brainstorming or generating creative ideas": "Brainstorming or generating creative ideas",
    "drafting professional text": "Drafting professional text (e.g., résumés, emails)",
    "résumés": "Drafting professional text (e.g., résumés, emails)",
    "emails": "Drafting professional text (e.g., résumés, emails)",
    "converting content between formats": "Converting content between formats (e.g., LaTeX)",
    "latex": "Converting content between formats (e.g., LaTeX)",
}

11.19
data_process.py
│
├── Constants & Config
│   ├── STATIC_EMBED_PATH / USE_BIGRAMS / USE_MULTI_INTERACTIONS
│   ├── TEXT_COLS / RATING_COLS / MULTI_COLS
│   ├── TEXT_VOCAB_PATH / MULTI_VOCAB_PATH
│   └── STOPWORDS / CANONICAL_MULTI_TYPES / MULTI_MAP
│
├── Train / Val / Test Split
│   └── train_val_test_split()
│
├── Text Processing
│   ├── normalize_to_text_list()
│   ├── load_static_embeddings()
│   ├── generate_ngrams()
│   ├── build_static_embedding_features()
│   ├── build_text_vocabulary()
│   └── build_text_matrix()
│
├── Rating Processing
│   ├── extract_rating()
│   ├── compute_rating_mode()
│   └── build_rating_matrix()
│
├── Multi-select Processing
│   ├── parse_multiselect()
│   ├── build_multiselect_vocabulary()
│   ├── build_multiselect_matrix()
│   └── build_multiselect_interaction_features()
│
├── Extra Features
│   ├── build_text_length_features()
│   └── build_multiselect_count_features()
│
├── Feature Assembly
│   └── build_feature_matrix()
│
├── Preprocessing API
│   ├── preprocess_train()
│   └── preprocess_test()
│
└── Main Entry
    └── main()

X = [BoW | rating | multi | multi_inter | text_len | multi_cnt | embedding]

# Feature switches（特征开关）
# 每个变量控制某一类特征是否加入最终的特征矩阵 X

USE_BOW = True
# 是否启用文本 Bag-of-Words（词袋）特征
# 作用：将自由文本题转成词频向量，是最主要的文本特征
# 特点：维度较大

USE_RATING = True
# 是否启用评分题（1-5 分）特征
# 作用：将类似“3 - Sometimes”转成数字并归一化到 0–1

USE_MULTI = True
# 是否启用多选题 one-hot 特征
# 作用：将多选题选项映射为 0/1 向量（例如是否选择了某项任务）

USE_MULTI_INTERACTIONS = False
# 是否加入多选题选项之间的 pairwise 交互特征
# 作用：例如同时选择“数学计算”与“写代码”会产生一个新特征

USE_TEXT_LEN = True
# 是否加入文本回答长度特征
# 作用：每个文本回答的 token 数量（log 压缩）

USE_MULTI_CNT = True
# 是否加入多选题选择项的数量特征
# 作用：每个多选题中选了多少项（log 压缩）

USE_STATIC_EMBEDDINGS = False
# 是否加入静态词向量（如 GloVe / FastText）的文本平均 embedding
# 作用：对每段文本做词向量平均，捕获语义信息


We develop a classifier that predicts whether ChatGPT, Claude, or Gemini produced each student survey response. The dataset contains 825 samples, each consisting of free-text descriptions, 1–5 rating items, and multi-select task-type indicators. We evaluate three model families: a neural network trained on TF–IDF features, a Naive Bayes model using the same representation, and a Random Forest trained on an engineered feature space combining bag-of-words text counts, encoded ratings, and multi-select indicators; we also include a simple k-Nearest Neighbors baseline for comparison. Our final deployed model is a RandomForestClassifier, reproduced for inference using a custom ManualRandomForest implementation. It achieves 0.92 training accuracy, 0.75 validation accuracy, and an estimated 0.70 test accuracy. Random Forests perform best because they capture nonlinear interactions across heterogeneous features and provide the most stable validation performance among all models evaluated.
We developed a classifier that predicts which large language model
(ChatGPT, Claude, or Gemini) produced each student survey response.
The dataset contains 825 responses, with three responses collected
from every student. Each response includes free-text descriptions,
1--5 rating questions, and several multi-select task-type indicators.

We evaluated three main model families. The first is a neural network
trained on TF--IDF features. The second is a Naive Bayes model that
uses the same TF--IDF representation. The third is a Random Forest
trained on an engineered feature space that combines bag-of-words
text counts, encoded rating values, and multi-select indicators.
We also tested a simple k-Nearest Neighbors baseline that relies only
on numeric and categorical features.

Our final deployed model is a RandomForestClassifier, which we
reproduced in the \texttt{ManualRandomForest} implementation for
inference. The model achieves 0.92 training accuracy, 0.75 validation
accuracy, and an estimated 0.70 test accuracy. Random Forests provided
the strongest results because they capture nonlinear interactions
across heterogeneous features, avoid the independence assumptions of
Naive Bayes, do not rely on the linear structure required by Logistic
Regression, and demonstrate the most stable validation performance
across all models we evaluated.


% ================================